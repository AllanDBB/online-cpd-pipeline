% =============================================================================
% SECCIÓN DE RESULTADOS PARA temp.tex
% Benchmark de Algoritmos de Detección de Change Points
% =============================================================================

\section{Results}

This section presents the experimental results from three complementary benchmarks: controlled synthetic data, manually annotated real crime data, and the TCPD repository. Each benchmark evaluates the 17 algorithms under different conditions, providing comprehensive insights into their performance characteristics, strengths, and limitations.

\subsection{Benchmark 1: Synthetic Data Performance}

The synthetic benchmark evaluated 1,008 algorithm-hyperparameter configurations across 120 time series (60 training, 60 test), systematically varying change type (escalón vs. pendiente), noise level (high vs. low), and change magnitude (high vs. low).

\subsubsection{Overall Algorithm Ranking}

Table~\ref{tab:ranking_synthetic} presents the top-performing algorithms ranked by mean F1-score on the test set. State-space models from the Canary library (SSM, TAGI-LSTM) achieved the highest overall performance, followed by Changepoint-Online methods and statistical control charts.

\begin{table}[H]
\caption{Top 10 algorithms on synthetic data benchmark (test set performance).\label{tab:ranking_synthetic}}
\begin{tabularx}{\textwidth}{lXXXX}
\toprule
\textbf{Algorithm} & \textbf{F1-Score} & \textbf{Precision} & \textbf{Recall} & \textbf{MTTD (steps)} \\
\midrule
SSM-Canary & 0.3897 & 0.4095 & 0.4151 & 4.13 \\
Gaussian (PELT) & 0.3801 & 0.3159 & 0.6562 & 3.91 \\
Two-Sample Tests & 0.3386 & 0.2534 & 0.6619 & 4.56 \\
Page-Hinkley & 0.3267 & 0.2674 & 0.6442 & 4.54 \\
TAGI-LSTM-SSM & 0.3225 & 0.3445 & 0.3413 & 4.49 \\
Neural Networks & 0.3100 & 0.3113 & 0.3750 & 4.63 \\
EWMA-NumPy & 0.3062 & 0.2180 & 0.7043 & 3.77 \\
EWMA-OCPDet & 0.2910 & 0.2074 & 0.7468 & 3.88 \\
CUSUM & 0.2853 & 0.1790 & 0.9119 & 5.26 \\
MDFocus (PELT) & 0.2821 & 0.3646 & 0.2588 & 4.19 \\
\bottomrule
\end{tabularx}
\end{table}

\textbf{Key findings:}
\begin{itemize}
    \item \textbf{SSM-Canary} dominated with F1=0.39, achieving the best balance between precision (0.41) and recall (0.42).
    \item \textbf{CUSUM} exhibited the highest recall (0.91) but lowest precision (0.18), indicating aggressive detection with many false positives.
    \item \textbf{EWMA variants} (NumPy and OCPDet) showed strong recall ($>0.70$) with fast detection (MTTD $<$ 4 steps).
    \item \textbf{Bayesian OCPD} failed completely (F1=0.00), detecting no change points across all series.
\end{itemize}

\subsubsection{Performance by Category}

Figure~\ref{fig:category_performance} illustrates performance stratification across the 8 experimental categories. Table~\ref{tab:best_by_category} identifies the winning algorithm for each condition.

\begin{table}[H]
\caption{Best algorithm per category (F1-score).\label{tab:best_by_category}}
\begin{tabularx}{\textwidth}{llllXXX}
\toprule
\textbf{Change} & \textbf{Noise} & \textbf{Magnitude} & \textbf{Winner} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} \\
\midrule
\multicolumn{7}{l}{\textbf{Step Changes (Escalón)}} \\
\midrule
Escalón & High & High & Two-Sample Tests & 0.498 & 0.406 & 0.808 \\
Escalón & High & Low & Gaussian (PELT) & 0.313 & 0.213 & 0.731 \\
Escalón & Low & High & SSM-Canary & \textbf{1.000} & \textbf{1.000} & \textbf{1.000} \\
Escalón & Low & Low & SKF-Canary & 0.769 & 0.872 & 0.712 \\
\midrule
\multicolumn{7}{l}{\textbf{Slope Changes (Pendiente)}} \\
\midrule
Pendiente & High & High & Gaussian (PELT) & 0.341 & 0.244 & 0.603 \\
Pendiente & High & Low & Gaussian (PELT) & 0.275 & 0.207 & 0.545 \\
Pendiente & Low & High & SSM-Canary & 0.480 & 0.462 & 0.564 \\
Pendiente & Low & Low & Two-Sample Tests & 0.440 & 0.313 & 0.878 \\
\bottomrule
\end{tabularx}
\end{table}

\textbf{Critical observations:}
\begin{itemize}
    \item \textbf{Perfect detection}: SSM-Canary achieved F1=1.00 on low-noise, high-magnitude step changes, demonstrating perfect change point localization under ideal conditions.
    \item \textbf{Noisy conditions}: Performance degraded significantly with high noise. Best F1-scores ranged from 0.27--0.50, with Two-Sample Tests and Gaussian PELT most robust.
    \item \textbf{Change type impact}: Step changes (escalón) were generally easier to detect (mean F1=0.42) than slope changes (pendiente, mean F1=0.19), likely due to abrupt transitions being more distinguishable.
    \item \textbf{Weak changes}: Low-magnitude changes in high-noise conditions proved extremely challenging, with best F1 $<$ 0.32.
\end{itemize}

\subsubsection{Noise Sensitivity Analysis}

Performance stratification by noise level revealed stark differences:

\begin{itemize}
    \item \textbf{Low noise (NSR $\in [0.0, 0.4]$)}: Mean F1=0.38 across algorithms
    \item \textbf{High noise (NSR $\in [3.0, 6.0]$)}: Mean F1=0.18 (53\% degradation)
\end{itemize}

Top performers under high noise:
\begin{enumerate}
    \item Page-Hinkley (F1=0.31)
    \item Two-Sample Tests (F1=0.26)
    \item CUSUM (F1=0.27)
\end{enumerate}

Statistical control charts (Page-Hinkley, CUSUM, EWMA) exhibited superior noise tolerance compared to model-based approaches (SSM, TAGI-LSTM), which suffered $>60\%$ F1-score drops.

\subsubsection{Library-Level Performance}

Aggregating results by implementation library (Table~\ref{tab:library_ranking}):

\begin{table}[H]
\caption{Average performance by implementation library.\label{tab:library_ranking}}
\begin{tabularx}{\textwidth}{lXXX}
\toprule
\textbf{Library} & \textbf{Mean F1} & \textbf{Mean Precision} & \textbf{Mean Recall} \\
\midrule
Canary (SSM) & 0.390 & 0.409 & 0.415 \\
Canary (TAGI-LSTM) & 0.322 & 0.344 & 0.341 \\
OCPDet & 0.306 & 0.238 & 0.674 \\
NumPy (EWMA) & 0.306 & 0.218 & 0.704 \\
Changepoint-Online & 0.295 & 0.341 & 0.346 \\
Canary (SKF) & 0.279 & 0.321 & 0.262 \\
River & 0.223 & 0.209 & 0.379 \\
SDAR (ChangeFinder) & 0.192 & 0.231 & 0.188 \\
Roerich (RULSIF) & 0.116 & 0.155 & 0.110 \\
CPFinder (BOCPD) & 0.000 & 0.000 & 0.000 \\
\bottomrule
\end{tabularx}
\end{table}

\textbf{Canary library} (state-space models) dominated with highest F1 and balanced precision-recall. \textbf{OCPDet} algorithms prioritized recall over precision, suitable for applications where missing detections are costly.

\subsection{Benchmark 2: Real Crime Data Performance}

The real-world benchmark evaluated algorithms on 50 manually labeled crime time series (25 training, 25 test) from Costa Rican criminal databases. Unlike synthetic data, these series contained natural noise patterns, irregular sampling artifacts, and realistic change point characteristics.

\subsubsection{Algorithm Ranking on Real Data}

Table~\ref{tab:ranking_real} presents performance on the real crime test set:

\begin{table}[H]
\caption{Top 10 algorithms on real crime data (test set).\label{tab:ranking_real}}
\begin{tabularx}{\textwidth}{lXXXXX}
\toprule
\textbf{Algorithm} & \textbf{F1-Score} & \textbf{Precision} & \textbf{Recall} & \textbf{MTTD} & \textbf{MMD} \\
\midrule
Gaussian (PELT) & 0.323 & 0.265 & 0.467 & 3.46 & 0.698 \\
CUSUM & 0.261 & 0.187 & 0.483 & 5.44 & 0.724 \\
Page-Hinkley & 0.257 & 0.197 & 0.443 & 6.23 & 0.703 \\
Two-Sample Tests & 0.256 & 0.250 & 0.293 & 5.08 & 0.545 \\
EWMA-OCPDet & 0.248 & 0.178 & 0.470 & 4.67 & 0.729 \\
EWMA-NumPy & 0.226 & 0.170 & 0.390 & 5.54 & 0.732 \\
Neural Networks & 0.213 & 0.267 & 0.223 & 5.75 & 0.794 \\
SSM-Canary & 0.187 & 0.187 & 0.240 & 4.55 & 0.800 \\
TAGI-LSTM-SSM & 0.179 & 0.160 & 0.240 & 4.55 & 0.785 \\
SKF-Canary & 0.155 & 0.160 & 0.160 & 4.29 & 0.947 \\
\bottomrule
\end{tabularx}
\end{table}

\subsubsection{Synthetic vs. Real Data Comparison}

Real-world performance was universally lower than synthetic benchmarks (Table~\ref{tab:synthetic_vs_real}):

\begin{table}[H]
\caption{Synthetic vs. real data F1-score comparison.\label{tab:synthetic_vs_real}}
\begin{tabularx}{\textwidth}{lXXX}
\toprule
\textbf{Algorithm} & \textbf{Synthetic F1} & \textbf{Real F1} & \textbf{Degradation} \\
\midrule
Gaussian (PELT) & 0.380 & 0.323 & -15.1\% \\
CUSUM & 0.285 & 0.261 & -8.7\% \\
Page-Hinkley & 0.327 & 0.257 & -21.4\% \\
Two-Sample Tests & 0.339 & 0.256 & -24.4\% \\
EWMA-OCPDet & 0.291 & 0.248 & -14.9\% \\
EWMA-NumPy & 0.306 & 0.226 & -26.3\% \\
Neural Networks & 0.310 & 0.213 & -31.2\% \\
SSM-Canary & 0.390 & 0.187 & -52.0\% \\
TAGI-LSTM-SSM & 0.322 & 0.179 & -44.5\% \\
SKF-Canary & 0.279 & 0.155 & -44.6\% \\
\bottomrule
\end{tabularx}
\end{table}

\textbf{Key insights:}
\begin{itemize}
    \item \textbf{State-space models} suffered largest degradation ($>44\%$), suggesting overfitting to synthetic patterns or sensitivity to modeling assumptions violated in real data.
    \item \textbf{Statistical control charts} (CUSUM, Page-Hinkley, EWMA) showed greater robustness with degradation $<27\%$.
    \item \textbf{Gaussian PELT} achieved best real-world performance (F1=0.32), maintaining relative stability across domains.
    \item \textbf{Bayesian OCPD} remained ineffective (F1=0.00), reinforcing its unsuitability for this problem class.
\end{itemize}

\subsection{Benchmark 3: TCPD Repository Evaluation}

The TCPD benchmark assessed algorithm robustness across 32 diverse real-world time series from various domains (finance, demographics, environmental, etc.) without ground truth labels. Evaluation focused on success rates, detection consistency, and runtime performance.

\subsubsection{Algorithm Robustness}

Table~\ref{tab:tcpd_robustness} summarizes success rates and efficiency:

\begin{table}[H]
\caption{Algorithm robustness on TCPD repository.\label{tab:tcpd_robustness}}
\begin{tabularx}{\textwidth}{lXXX}
\toprule
\textbf{Algorithm} & \textbf{Success Rate (\%)} & \textbf{Avg Runtime (s)} & \textbf{Avg Detections} \\
\midrule
\multicolumn{4}{l}{\textit{Perfect Reliability (100\% success)}} \\
\midrule
Page-Hinkley & 100.0 & 0.000 & 14.55 \\
CUSUM & 100.0 & 0.000 & 12.19 \\
Two-Sample Tests & 100.0 & 0.010 & 8.28 \\
EWMA (NumPy) & 100.0 & 0.000 & 4.84 \\
EWMA (OCPDet) & 100.0 & 0.000 & 4.84 \\
Neural Networks & 100.0 & 0.040 & 4.44 \\
TAGI-LSTM-SSM & 100.0 & 0.010 & 2.66 \\
SSM-Canary & 100.0 & 0.010 & 2.25 \\
RULSIF & 100.0 & 0.240 & 1.25 \\
SKF-Canary & 100.0 & 0.010 & 1.31 \\
ADWIN & 100.0 & 0.000 & 1.81 \\
BOCPD & 100.0 & 0.050 & 0.00 \\
ChangeFinder & 100.0 & 0.000 & 0.75 \\
NPFocus & 100.0 & 0.000 & 0.69 \\
\midrule
\multicolumn{4}{l}{\textit{Near-Perfect Reliability}} \\
\midrule
Focus (PELT) & 96.9 & 0.050 & 1.55 \\
MDFocus (PELT) & 96.9 & 0.010 & 1.00 \\
\midrule
\multicolumn{4}{l}{\textit{Occasional Failures}} \\
\midrule
Gaussian (PELT) & 93.8 & 0.020 & 6.30 \\
\bottomrule
\end{tabularx}
\end{table}

\textbf{Robustness analysis:}
\begin{itemize}
    \item \textbf{14 algorithms} achieved 100\% success rate across all 32 datasets, demonstrating production-grade reliability.
    \item \textbf{Gaussian PELT} encountered errors on 2 datasets (6.2\% failure rate), likely due to insufficient data length for the min\_size constraint.
    \item \textbf{Runtime efficiency}: Statistical methods (CUSUM, EWMA, Page-Hinkley) and lightweight models (ADWIN, ChangeFinder) completed in $<$1ms on average. RULSIF was slowest (240ms) due to neural density estimation.
    \item \textbf{Detection aggressiveness}: Page-Hinkley (14.6 detections/series) and CUSUM (12.2) were most sensitive, while BOCPD detected nothing (reinforcing synthetic findings).
\end{itemize}

\subsubsection{Detection Patterns Across Domains}

Table~\ref{tab:tcpd_datasets} shows detection statistics for selected datasets:

\begin{table}[H]
\caption{Detection patterns on TCPD datasets (top 10 by mean detections).\label{tab:tcpd_datasets}}
\begin{tabularx}{\textwidth}{lXXXXX}
\toprule
\textbf{Dataset} & \textbf{Length} & \textbf{Mean Det.} & \textbf{Std Dev} & \textbf{Min} & \textbf{Max} \\
\midrule
Brent Spot Price & 500 & 10.00 & 10.48 & 0 & 39 \\
JFK Passengers & 144 & 9.38 & 13.48 & 0 & 46 \\
Business Inventory & 330 & 8.46 & 9.46 & 0 & 33 \\
Bank Account & 581 & 7.79 & 8.61 & 0 & 24 \\
Construction & 216 & 6.71 & 9.01 & 0 & 31 \\
Children per Woman & 301 & 4.04 & 4.19 & 0 & 15 \\
Global CO$_2$ & 264 & 1.88 & 1.75 & 0 & 6 \\
GDP Argentina & 58 & 1.08 & 1.47 & 0 & 5 \\
GDP Japan & 66 & 0.96 & 1.52 & 0 & 5 \\
Centralia Population & 15 & 0.62 & 0.67 & 0 & 2 \\
\bottomrule
\end{tabularx}
\end{table}

Variability in detection counts (high standard deviations) reflects fundamental algorithmic differences: aggressive detectors (CUSUM, Page-Hinkley) versus conservative methods (BOCPD, SKF).

\subsection{Comparative Analysis and Insights}

\subsubsection{Algorithm Suitability by Context}

Based on the three benchmarks, we provide application-specific recommendations:

\begin{itemize}
    \item \textbf{Low-noise environments}: State-space models (SSM-Canary, TAGI-LSTM) excel with F1 $>$ 0.48, offering precise localization.
    \item \textbf{High-noise robustness}: Statistical control charts (Page-Hinkley, CUSUM, Two-Sample Tests) maintain performance with minimal degradation.
    \item \textbf{Real-world deployments}: Gaussian PELT balances synthetic (F1=0.38) and real (F1=0.32) performance with high reliability (93.8\% TCPD success).
    \item \textbf{Production systems}: ADWIN, EWMA, and CUSUM combine 100\% reliability, sub-millisecond runtime, and moderate detection sensitivity.
    \item \textbf{Conservative detection}: SKF-Canary and ChangeFinder minimize false positives, suitable for costly intervention scenarios.
\end{itemize}

\subsubsection{Failure Mode Analysis}

\textbf{Bayesian OCPD (CPFinder)} consistently failed across all benchmarks (F1=0.00, 0 detections). Possible causes:
\begin{enumerate}
    \item Hyperparameter space insufficiently explored (hazard\_lambda, probability\_threshold)
    \item Implementation incompatibility with univariate Gaussian assumptions
    \item Numerical instability in posterior computation for these data characteristics
\end{enumerate}

\textbf{Gaussian PELT} errors on short TCPD series suggest min\_size parameter requires dynamic adjustment based on series length.

\subsubsection{Performance-Complexity Trade-offs}

Figure~\ref{fig:performance_vs_complexity} (not shown) would plot F1-score vs. average runtime, revealing:
\begin{itemize}
    \item \textbf{Pareto-optimal}: EWMA (F1=0.31, $<$1ms) and Two-Sample Tests (F1=0.34, 10ms) offer best performance per computational cost.
    \item \textbf{Diminishing returns}: State-space models' marginal F1 gain ($+$5\%) incurs 10-50× runtime penalty.
    \item \textbf{Heavyweight methods}: RULSIF (240ms) underperforms simpler baselines despite neural density estimation sophistication.
\end{itemize}

\subsection{Summary of Key Findings}

\begin{enumerate}
    \item \textbf{Best overall algorithm}: SSM-Canary (F1=0.39 synthetic, 100\% TCPD reliability) for controlled environments; Gaussian PELT (F1=0.32 real) for practical deployments.
    \item \textbf{Noise tolerance}: Statistical control charts (Page-Hinkley, CUSUM, EWMA) outperform model-based methods under high noise ($>$50\% F1 retention vs. $<$40\%).
    \item \textbf{Change type sensitivity}: Step changes (escalón) 2× easier to detect than slope changes (pendiente) across all algorithms.
    \item \textbf{Real-world gap}: 15-52\% F1 degradation from synthetic to real data, with state-space models most affected.
    \item \textbf{Robustness leaders}: 14/17 algorithms achieved 100\% TCPD success rate, demonstrating mature implementations.
    \item \textbf{Failed methods}: Bayesian OCPD ineffective for this problem class; requires further investigation or hyperparameter tuning.
\end{enumerate}

These results provide empirical guidance for practitioners selecting change point detection algorithms based on application constraints (noise level, change type, real-time requirements) and acceptable performance trade-offs.
