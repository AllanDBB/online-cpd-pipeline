\section{Materials and Methods}
\label{sec:methods}

This section describes the experimental framework designed to evaluate online change point detection algorithms under three complementary evaluation paradigms: synthetic data generation, real-world labeled data, and transfer learning validation. Our methodology follows a rigorous train-test protocol with comprehensive hyperparameter optimization and multi-metric evaluation.

\subsection{Experimental Framework Overview}

The evaluation framework consists of three interconnected benchmarking pipelines:

\begin{enumerate}
    \item \textbf{Benchmark 1: Synthetic Data Evaluation} -- Controlled environment with known ground truth, enabling systematic assessment across noise levels, change magnitudes, and change types.
    \item \textbf{Benchmark 2: Real-World Data Evaluation} -- Manual labeled crime statistics from Costa Rica, providing domain-specific validation.
    \item \textbf{Benchmark 3: Transfer Learning Validation} -- Investigates parameter transferability from synthetic to real domains.
\end{enumerate}

All benchmarks employ identical evaluation metrics and train/test methodology to ensure fair comparison.

\subsection{Datasets}

\subsubsection{Synthetic Time Series (Benchmark 1)}
\label{sec:synthetic_data}

We generate synthetic time series with controlled characteristics to systematically evaluate algorithm performance under various conditions.

\paragraph{Time Series Generation}

Each synthetic series is generated using an autoregressive process with injected change points:

\begin{equation}
x_t = \phi x_{t-1} + \mu_s + \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0, \sigma^2)
\end{equation}

where:
\begin{itemize}
    \item $\phi = 0.3$ (autoregressive coefficient)
    \item $\mu_s$ is the segment mean (changes at change points)
    \item $\sigma$ controls noise level
\end{itemize}

\paragraph{Experimental Design}

We employ a full factorial design covering:

\begin{itemize}
    \item \textbf{Noise Levels:} 
    \begin{itemize}
        \item Low: $\sigma \in [0.0, 0.4]$ (high SNR)
        \item High: $\sigma \in [3.0, 6.0]$ (low SNR)
    \end{itemize}
    
    \item \textbf{Change Magnitudes:}
    \begin{itemize}
        \item Low: $|\Delta\mu| \in [0.5, 1.5]$ standard deviations
        \item High: $|\Delta\mu| \in [3.0, 6.0]$ standard deviations
    \end{itemize}
    
    \item \textbf{Change Types:}
    \begin{itemize}
        \item Step (escal√≥n): Abrupt mean shift
        \item Slope (pendiente): Gradual linear trend change
    \end{itemize}
    
    \item \textbf{Series Characteristics:}
    \begin{itemize}
        \item Length: $L \in \{200, 300, 400\}$ time steps
        \item Number of change points: $n_{cp} \in \{1, 2, 3, 4\}$
    \end{itemize}
\end{itemize}

This yields $2 \times 2 \times 2 = 8$ unique scenario combinations. For each scenario, we generate 45 series (3 iterations $\times$ 15 series per iteration), resulting in \textbf{360 total series}.

\paragraph{Train-Test Split}

Series are randomly partitioned into:
\begin{itemize}
    \item Training set: 70\% (252 series) -- used for hyperparameter optimization
    \item Test set: 30\% (108 series) -- used for final evaluation
\end{itemize}

Random seed is fixed ($seed=123$) to ensure reproducibility.

\subsubsection{Real-World Crime Data (Benchmark 2)}
\label{sec:real_data}

We used manually labeled time series of crime statistics from Costa Rica and elsewhere, which provides domain-specific validation of the algorithm's performance. These series were labeled using a tool we developed, which can be accessed at: https://dcp-itcr.space/

\paragraph{Dataset Description}

\begin{itemize}
    \item \textbf{Domain:} Monthly crime incident counts by category and region
    \item \textbf{Series Count:} 49 series (labeled by primary annotator Martin)
    \item \textbf{Change Points:} Manually annotated by domain expert
    \item \textbf{Annotation Metadata:} Change type, confidence level, contextual notes
\end{itemize}

\paragraph{Inter-Annotator Agreement}

To assess labeling reliability, we computed agreement between two independent annotators using F1 score with tolerance $\delta=10$:

\begin{equation}
\text{Agreement F1} = 0.24
\end{equation}

This low agreement reflects the inherent ambiguity in change point annotation for real-world data. Following best practices, we use labels from the primary domain expert (Martin) exclusively to avoid introducing inconsistent training signals.

\paragraph{Series Classification}

Real series are automatically classified before benchmarking to enable stratified analysis. Classification uses two criteria:

\textbf{1. Noise Level Classification:}

We estimate noise using Noise-to-Signal Ratio (NSR):

\begin{equation}
\text{NSR} = \frac{\text{Var}(\text{noise})}{\text{Var}(\text{signal})}
\end{equation}

where signal is estimated via Savitzky-Golay smoothing filter (window=5\% of series length, polyorder=2). Series are classified as:
\begin{itemize}
    \item Low noise: NSR $< \text{median(NSR)}$
    \item High noise: NSR $\geq \text{median(NSR)}$
\end{itemize}

\textbf{2. Change Magnitude Classification:}

Change magnitude is computed as the minimum mean difference between consecutive segments defined by labeled change points:

\begin{equation}
\Delta_{\text{min}} = \min_{i} |\mu_i - \mu_{i-1}|
\end{equation}

where $\mu_i$ is the mean of segment $i$. Classification:
\begin{itemize}
    \item Low magnitude: $\Delta_{\text{min}} < \text{median}(\Delta_{\text{min}})$
    \item High magnitude: $\Delta_{\text{min}} \geq \text{median}(\Delta_{\text{min}})$
\end{itemize}

This stratification enables analysis of algorithm performance across natural difficulty levels.

\paragraph{Train-Test Split}

Series are randomly partitioned into:
\begin{itemize}
    \item Training set: 50\% (24-25 series)
    \item Test set: 50\% (24-25 series)
\end{itemize}

The 50-50 split (rather than 70-30) is used due to limited total series count, balancing hyperparameter optimization capability with robust test set evaluation.

\subsection{Evaluated Algorithms}
\label{sec:algorithms}

We evaluate 17 online change point detection algorithms spanning multiple methodological families:

\subsubsection{Statistical Process Control}

\begin{itemize}
    \item \textbf{Page-Hinkley (PH)} \cite{page1954continuous}: Sequential likelihood ratio test tracking cumulative deviation from baseline mean.
    
    \item \textbf{ADWIN} \cite{bifet2007learning}: Adaptive Windowing algorithm maintaining statistical window of recent observations.
    
    \item \textbf{EWMA} \cite{roberts1959control}: Exponentially Weighted Moving Average control chart with dynamic threshold.
    
    \item \textbf{CUSUM} \cite{page1954continuous}: Cumulative Sum control chart detecting persistent shifts from target.
\end{itemize}

\subsubsection{Segmentation-Based Methods}

\begin{itemize}
    \item \textbf{Focus (RBF)} \cite{truong2020selective}: Kernel-based PELT algorithm using radial basis function cost.
    
    \item \textbf{Gaussian (L2)} \cite{truong2020selective}: PELT with Gaussian likelihood (L2 loss).
    
    \item \textbf{NPFocus} \cite{truong2020selective}: Non-parametric change detection via sliding window comparison.
    
    \item \textbf{MDFocus} \cite{truong2020selective}: Multivariate extension using Mahalanobis distance.
\end{itemize}

\subsubsection{State-Space Models}

\begin{itemize}
    \item \textbf{SSM-Canary} \cite{canary2023}: Basic Kalman filter monitoring prediction residuals.
    
    \item \textbf{TAGI-LSTM} \cite{tagi2023}: Tractable Approximate Gaussian Inference with LSTM architecture for adaptive state tracking.
    
    \item \textbf{SKF-Canary} \cite{canary2023}: Robust Square Root Kalman Filter with outlier handling.
\end{itemize}

\subsubsection{Bayesian Methods}

\begin{itemize}
    \item \textbf{BCPD (CPFinder)} \cite{adams2007bayesian}: Bayesian Online Change Point Detection with Student-t predictive distribution.
\end{itemize}

\subsubsection{Neural Network-Based}

\begin{itemize}
    \item \textbf{OCPDet-Neural} \cite{ocpdet2022}: Autoregressive MLP monitoring prediction residuals.
    
    \item \textbf{RuLSIF (Roerich)} \cite{roerich2021}: Relative Unconstrained Least-Squares Importance Fitting via neural density ratio estimation.
\end{itemize}

\subsubsection{Two-Sample Testing}

\begin{itemize}
    \item \textbf{Two-Sample Test (KS)} \cite{ross2011two}: Sliding window Kolmogorov-Smirnov test.
    
    \item \textbf{ChangeFinder} \cite{takeuchi2006unifying}: Sequential discounting autoregression with outlier scoring.
\end{itemize}

All algorithms are implemented in their respective reference libraries (River, Ruptures, OCPDet, etc.) using default or tuned hyperparameters as specified in Section~\ref{sec:hyperparameter_opt}.

\subsection{Evaluation Metrics}
\label{sec:metrics}

We employ four complementary metrics to assess algorithm performance:

\subsubsection{F1 Score with Temporal Tolerance}

Following standard practice in time series change point detection \cite{arlot2019kernel}, we use F1 score with tolerance window $\delta$ to account for acceptable detection delays:

\begin{equation}
\text{F1}_\delta = \frac{2 \cdot \text{Precision}_\delta \cdot \text{Recall}_\delta}{\text{Precision}_\delta + \text{Recall}_\delta}
\end{equation}

where:

\begin{align}
\text{Precision}_\delta &= \frac{\text{TP}_\delta}{\text{TP}_\delta + \text{FP}_\delta} \\
\text{Recall}_\delta &= \frac{\text{TP}_\delta}{\text{TP}_\delta + \text{FN}_\delta}
\end{align}

A detected change point $\hat{t}$ is considered a True Positive (TP) if:
\begin{equation}
\exists t^* \in \mathcal{T}_{\text{true}} : |t^* - \hat{t}| \leq \delta
\end{equation}

where $\mathcal{T}_{\text{true}}$ is the set of ground truth change points.

We use $\delta = 10$ time steps, allowing algorithms to detect changes within a 10-step window before/after ground truth.

\subsubsection{Maximum Mean Discrepancy (MMD)}

MMD measures distributional similarity between ground truth and detected change point sets:

\begin{equation}
\text{MMD}(\mathcal{T}_{\text{true}}, \mathcal{T}_{\text{det}}) = \sup_{f \in \mathcal{F}} \left| \mathbb{E}_{t \sim \mathcal{T}_{\text{true}}}[f(t)] - \mathbb{E}_{\hat{t} \sim \mathcal{T}_{\text{det}}}[f(\hat{t})] \right|
\end{equation}

We use Gaussian kernel MMD implementation from \cite{gretton2012kernel}. Lower values indicate better temporal alignment.

\subsubsection{Mean Time to Detection (MTTD)}

For applications requiring rapid response, we compute average detection delay:

\begin{equation}
\text{MTTD} = \frac{1}{|\text{TP}_\delta|} \sum_{t^* \in \mathcal{T}_{\text{true}}} \min_{\hat{t} \in \mathcal{T}_{\text{det}}} |\hat{t} - t^*|
\end{equation}

conditioned on $|\hat{t} - t^*| \leq \delta$. Lower values indicate faster detection.

\subsubsection{Detection Count Statistics}

We report mean number of detected change points per series to identify over/under-detection tendencies:

\begin{equation}
\overline{n}_{\text{det}} = \frac{1}{N} \sum_{i=1}^{N} |\mathcal{T}_{\text{det}}^{(i)}|
\end{equation}

\subsection{Hyperparameter Optimization}
\label{sec:hyperparameter_opt}

Each algorithm has a predefined hyperparameter grid for exhaustive search. We employ exhaustive grid search with train-test methodology:

\paragraph{Grid Search Protocol}

\begin{enumerate}
    \item \textbf{Training Phase:}
    \begin{enumerate}
        \item Enumerate all parameter combinations $\Theta = \{\theta_1, \ldots, \theta_K\}$
        \item For each $\theta_k$:
        \begin{itemize}
            \item Apply algorithm to all training series
            \item Compute $\text{F1}_{\delta}$ for each series
            \item Calculate mean F1: $\overline{\text{F1}}_{\text{train}}(\theta_k)$
        \end{itemize}
        \item Select best parameters: $\theta^* = \arg\max_{\theta_k} \overline{\text{F1}}_{\text{train}}(\theta_k)$
    \end{enumerate}
    
    \item \textbf{Test Phase:}
    \begin{enumerate}
        \item Apply $\theta^*$ to all test series
        \item Compute all metrics (F1, Precision, Recall, MMD, MTTD)
        \item Report test performance: $\overline{\text{F1}}_{\text{test}}(\theta^*)$
    \end{enumerate}
\end{enumerate}

\paragraph{Timeout Handling}

To prevent computational deadlock, each algorithm has a 30-second timeout per series. Timeout failures are recorded and excluded from metric calculations.

\subsection{Benchmark 1: Synthetic Data Evaluation}
\label{sec:benchmark1}

\paragraph{Objective}

Assess algorithm performance under controlled conditions with known ground truth across systematic variations in:
\begin{itemize}
    \item Signal-to-noise ratio
    \item Change point magnitude
    \item Change point type (abrupt vs gradual)
\end{itemize}

\paragraph{Procedure}

\begin{enumerate}
    \item Generate 360 synthetic series (8 scenarios $\times$ 45 series each) as described in Section~\ref{sec:synthetic_data}
    \item Split into train (252) and test (108) sets
    \item For each of 17 algorithms:
    \begin{enumerate}
        \item Perform grid search on training set
        \item Select best parameters $\theta^*$
        \item Evaluate on test set with $\theta^*$
        \item Record all metrics
    \end{enumerate}
    \item Analyze performance by scenario (noise $\times$ magnitude $\times$ type)
\end{enumerate}

\paragraph{Output}

\begin{itemize}
    \item Primary: Test set F1 scores per algorithm per scenario
    \item Secondary: Precision, recall, MMD, MTTD per algorithm
    \item Metadata: Best parameters $\theta^*$ per algorithm per scenario
\end{itemize}

\subsection{Benchmark 2: Real-World Data Evaluation}
\label{sec:benchmark2}

\paragraph{Objective}

Validate algorithm performance on domain-specific real-world data with natural noise characteristics and expert-labeled change points.

\paragraph{Procedure}

\begin{enumerate}
    \item Load 49 labeled crime series (Section~\ref{sec:real_data})
    \item Classify series by noise level and change magnitude
    \item Split into train (24-25) and test (24-25) sets
    \item For each algorithm:
    \begin{enumerate}
        \item Perform grid search on training set
        \item Select best parameters $\theta^*$
        \item Evaluate on test set with $\theta^*$
        \item Record all metrics
    \end{enumerate}
    \item Analyze performance by series classification category
\end{enumerate}

\paragraph{Output}

\begin{itemize}
    \item Primary: Test set F1 scores per algorithm
    \item Secondary: Precision, recall, MMD, MTTD per algorithm
    \item Stratified: Performance breakdown by noise and magnitude categories
    \item Metadata: Series classification results
\end{itemize}

\subsection{Benchmark 3: Transfer Learning Validation}
\label{sec:benchmark3}

\paragraph{Objective}

Investigate whether hyperparameters optimized on synthetic data transfer effectively to real-world data, enabling rapid algorithm evaluation without expensive grid search.

\paragraph{Research Questions}

\begin{enumerate}
    \item \textbf{RQ1:} Do synthetic-optimized parameters $\theta^*_{\text{syn}}$ perform comparably to real-optimized parameters $\theta^*_{\text{real}}$ on real data?
    \item \textbf{RQ2:} Which algorithms are most robust to domain shift (synthetic $\rightarrow$ real)?
    \item \textbf{RQ3:} Does synthetic performance correlate with real performance?
\end{enumerate}

\paragraph{Procedure}

\begin{enumerate}
    \item Extract best parameters from Benchmark 1: $\Theta^*_{\text{syn}} = \{\theta^*_{\text{syn},1}, \ldots, \theta^*_{\text{syn},17}\}$
    \item For each algorithm:
    \begin{enumerate}
        \item Apply $\theta^*_{\text{syn}}$ directly to real training set (no optimization)
        \item Evaluate on real test set
        \item Record all metrics
    \end{enumerate}
    \item Compare with Benchmark 2 results:
    \begin{itemize}
        \item Compute $\Delta \text{F1} = \text{F1}_{\text{transfer}} - \text{F1}_{\text{grid}}$
        \item Calculate correlation: $\rho(\text{F1}_{\text{syn}}, \text{F1}_{\text{real}})$
        \item Identify algorithms with robust transfer ($|\Delta \text{F1}| < 0.05$)
    \end{itemize}
\end{enumerate}

\paragraph{Output}

\begin{itemize}
    \item Primary: Transfer learning test F1 vs grid search test F1 comparison
    \item Analysis: Correlation between synthetic and real performance
    \item Recommendations: Algorithms suitable for rapid deployment (good transfer)
    \item Computational savings: Grid search time vs transfer learning time
\end{itemize}

\subsection{Reproducibility}
\label{sec:reproducibility}

All experiments are fully reproducible:
\begin{itemize}
    \item Random seeds fixed ($seed=123$)
    \item Code available at: \url{https://github.com/AllanDBB/online-cpd-pipeline}
    \item Datasets included in repository (synthetic generation scripts + real data CSVs)
    \item Requirements: Python 3.9+, dependencies listed in \texttt{requirements.txt}
    \item Execution time: ~2 hours (synthetic) + ~4 hours (real) + ~20 minutes (transfer) on standard laptop
\end{itemize}

All results are saved with timestamped filenames and include:
\begin{itemize}
    \item Configuration metadata (JSON)
    \item Per-series detailed results
    \item Aggregated summary statistics
    \item Best parameters per algorithm
\end{itemize}

\subsection{Statistical Analysis}
\label{sec:statistical_analysis}

We employ the following statistical tests:

\paragraph{Algorithm Comparison}

For pairwise algorithm comparison, we use:
\begin{itemize}
    \item Friedman test (non-parametric) to detect overall differences
    \item Post-hoc Nemenyi test for pairwise ranking
    \item Critical difference diagrams for visualization
\end{itemize}

\paragraph{Scenario Effect Analysis}

To assess impact of noise, magnitude, and change type:
\begin{itemize}
    \item Three-way ANOVA (or Kruskal-Wallis if non-normal)
    \item Effect size quantification ($\eta^2$)
    \item Post-hoc multiple comparison correction (Bonferroni)
\end{itemize}

\paragraph{Transfer Learning Validation}

\begin{itemize}
    \item Pearson correlation between synthetic and real performance
    \item Paired t-test: grid search vs transfer learning
    \item Wilcoxon signed-rank test (non-parametric alternative)
\end{itemize}

Statistical significance threshold: $\alpha = 0.05$ (Bonferroni-corrected for multiple comparisons).
